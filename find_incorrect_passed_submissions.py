import os
import polars as pl
from submitter import LocalCodeSubmitter
import gc
import tempfile
import subprocess
import platform
import shutil
import json
from typing import Optional
from multiprocessing import Pool, cpu_count
import pickle
from functools import partial
import psutil  # æ·»åŠ å†…å­˜ç›‘æ§

dataset_path = "./dataset/ccplus_1x"
submitter = LocalCodeSubmitter()

# è¾“å‡ºç›®å½•
output_dir = "output_passed_incorrect"
checkpoint_file = "checkpoint.json"

# åˆ›å»ºè¾“å‡ºç›®å½•
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"âœ“ Created output directory: {output_dir}")  # æ–­ç‚¹ç»­ä¼ æ–‡ä»¶

# æ‰¹é‡å¤„ç†å¤§å°
BATCH_SIZE = 100

# å¹¶è¡Œå¤„ç†é…ç½®
USE_MULTIPROCESSING = False  # æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹
NUM_WORKERS = 5

# å†…å­˜ç®¡ç†é…ç½®
MAX_PROCESSED_PROBLEMS_IN_MEMORY = 10000  # processed_problemsè¾¾åˆ°æ­¤æ•°é‡æ—¶å†™å…¥ç£ç›˜å¹¶æ¸…ç©º
MEMORY_WARNING_THRESHOLD_MB = 8000  # å†…å­˜ä½¿ç”¨è¶…è¿‡æ­¤å€¼æ—¶å‘å‡ºè­¦å‘Šï¼ˆ8GBï¼‰

def get_memory_usage_mb():
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def check_memory_usage():
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨ï¼Œå¦‚æœè¶…è¿‡é˜ˆå€¼åˆ™è­¦å‘Š"""
    mem_mb = get_memory_usage_mb()
    if mem_mb > MEMORY_WARNING_THRESHOLD_MB:
        print(f"âš  WARNING: Memory usage is high: {mem_mb:.1f} MB")
        print(f"  Consider reducing BATCH_SIZE or enabling checkpointing more frequently")
    return mem_mb

# Compilation parameters
GPP = "g++"
CXX_STANDARDS = ["c++23", "c++20", "c++17", "c++14", "c++11"]
SYSTEM = platform.system()

# ç¼“å­˜å·²ç¼–è¯‘çš„checkerï¼Œé¿å…é‡å¤ç¼–è¯‘
checker_cache = {}  # {checker_code_hash: (is_compilable, exe_path or None)}
checker_cache_file = "checker_cache.pkl"  # checkerç¼“å­˜æŒä¹…åŒ–æ–‡ä»¶

def load_checker_cache():
    """åŠ è½½checkerç¼“å­˜"""
    global checker_cache
    if os.path.exists(checker_cache_file):
        try:
            with open(checker_cache_file, 'rb') as f:
                checker_cache = pickle.load(f)
            print(f"âœ“ Loaded checker cache: {len(checker_cache)} entries")
        except Exception as e:
            print(f"âš  Failed to load checker cache: {e}")
            checker_cache = {}

def save_checker_cache():
    """ä¿å­˜checkerç¼“å­˜"""
    try:
        with open(checker_cache_file, 'wb') as f:
            pickle.dump(checker_cache, f)
        print(f"âœ“ Saved checker cache: {len(checker_cache)} entries")
    except Exception as e:
        print(f"âš  Failed to save checker cache: {e}")

def get_code_hash(code: str) -> str:
    """è·å–ä»£ç çš„hashå€¼ç”¨äºç¼“å­˜"""
    import hashlib
    return hashlib.md5(code.encode()).hexdigest() if code else ""

def load_checkpoint():
    """åŠ è½½æ–­ç‚¹ä¿¡æ¯"""
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
            print(f"âœ“ Resuming from checkpoint: {checkpoint}")
            return checkpoint
        except Exception as e:
            print(f"âš  Failed to load checkpoint: {e}")
    return {"processed_problems": [], "total_saved": 0}

def save_checkpoint(processed_problems: list, total_saved: int):
    """ä¿å­˜æ–­ç‚¹ä¿¡æ¯"""
    try:
        checkpoint = {
            "processed_problems": processed_problems,
            "total_saved": total_saved,
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)
    except Exception as e:
        print(f"âš  Failed to save checkpoint: {e}")

def get_compile_flags(std: str) -> list[str]:
    """Get compilation flags based on C++ standard."""
    base_flags = ["-I.", "-DONLINE_JUDGE", "-O2"]

    if SYSTEM == "Darwin":  # macOS
        flags = base_flags
    else:  # Linux or other Unix-like systems
        static_flags = ["-static"]
        flags = static_flags + base_flags


    return flags

def can_compile_code(code: str, prefix: str = "test_") -> bool:
    """Test if a C++ code can compile successfully."""
    if not code or code.strip() == "":
        return False

    temp_dir = tempfile.mkdtemp(prefix=prefix)
    try:
        src_path = os.path.join(temp_dir, "main.cpp")
        exe_path = os.path.join(temp_dir, "main")

        # Write code to file
        with open(src_path, "w", encoding="utf-8") as f:
            f.write(code)

        # Try to compile with different C++ standards
        for std in CXX_STANDARDS:
            compile_flags = get_compile_flags(std)
            compile_command = [GPP, f"-std={std}", *compile_flags, src_path, "-o", exe_path]

            try:
                res = subprocess.run(
                    compile_command,
                    capture_output=True,
                    text=True,
                    timeout=30,
                )

                if res.returncode == 0 and os.path.exists(exe_path):
                    return True

            except (subprocess.TimeoutExpired, Exception):
                continue

        return False

    finally:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

def can_compile_checker(checker_code: Optional[str]) -> bool:
    """
    Test if a checker (SPJ) code can compile successfully.
    Returns True if the checker is None (traditional problem) or compiles successfully.
    Returns False if compilation fails.
    ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜é¿å…é‡å¤ç¼–è¯‘ç›¸åŒçš„checker
    """
    if checker_code is None or checker_code == "":
        # Traditional problem without SPJ - always valid
        return True

    # æ£€æŸ¥ç¼“å­˜
    checker_hash = get_code_hash(checker_code)
    if checker_hash in checker_cache:
        return checker_cache[checker_hash]

    # ç¼–è¯‘å¹¶ç¼“å­˜ç»“æœ
    result = can_compile_code(checker_code, prefix="checker_test_")
    checker_cache[checker_hash] = result
    return result

def get_first_correct_cpp_code(correct_submissions: list, problem_item: dict, submitter) -> Optional[str]:
    """
    ä»correct_submissionsä¸­è·å–ç¬¬ä¸€ä¸ªçœŸæ­£ACçš„C++ä»£ç 
    é€ä¸ªæäº¤éªŒè¯ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªé€šè¿‡æ‰€æœ‰æµ‹è¯•çš„ä»£ç åç«‹å³è¿”å›
    ä¼˜åŒ–ï¼šå…ˆå¿«é€Ÿæ£€æŸ¥ç¼–è¯‘ï¼Œç¼–è¯‘å¤±è´¥çš„ç›´æ¥è·³è¿‡

    Args:
        correct_submissions: æ­£ç¡®æäº¤åˆ—è¡¨
        problem_item: é¢˜ç›®æ•°æ®ï¼ˆåŒ…å«test_casesç­‰ï¼‰
        submitter: LocalCodeSubmitterå®ä¾‹

    Returns:
        ç¬¬ä¸€ä¸ªéªŒè¯é€šè¿‡çš„C++ä»£ç ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    if not correct_submissions:
        return None

    cpp_count = 0
    # é€ä¸ªæµ‹è¯•C++ä»£ç 
    for idx, submission in enumerate(correct_submissions):
        if submission.get("language") != 'cpp':
            continue

        code = submission.get("code")
        if not code or len(code.strip()) == 0:
            continue

        cpp_count += 1

        if not can_compile_code(code, prefix="correct_quick_"):
            print(f"  âœ— Submission #{cpp_count}: compilation failed")
            continue

        print(f"  éªŒè¯ correct submission #{cpp_count}...")

        # å•ä¸ªæäº¤éªŒè¯
        result = submitter.batch_submit_code(
            problem_item.get("id"),
            [code],  # åªæäº¤ä¸€ä¸ª
            problem_item,
            all_judge=False,
            original_result="correct"
        )

        # æ£€æŸ¥æ˜¯å¦é€šè¿‡
        if "passed_submissions" in result and len(result["passed_submissions"]) > 0:
            print(f"  âœ“ Found verified correct code ({len(code)} chars, tried {cpp_count} submissions)")
            return code
        else:
            print(f"  âœ— Failed tests")
            continue

    if cpp_count == 0:
        print(f"  âš  No C++ submissions found")
    else:
        print(f"  âš  No correct submission passed all tests (tried {cpp_count} submissions)")
    return None

def append_to_parquet(records: list, output_file: str):
    """
    è¿½åŠ è®°å½•åˆ°parquetæ–‡ä»¶
    ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¿«çš„å†™å…¥æ–¹å¼ï¼Œé¿å…æ¯æ¬¡éƒ½è¯»å–æ•´ä¸ªæ–‡ä»¶

    Args:
        records: è¦å†™å…¥çš„è®°å½•åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if not records:
        return

    new_df = pl.DataFrame(records)

    # ä¼˜åŒ–ï¼šå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥å†™å…¥
    if not os.path.exists(output_file):
        new_df.write_parquet(output_file, compression="zstd")
        del new_df
        gc.collect()
        return

    # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¿«çš„concatæ–¹å¼
    try:
        existing_df = pl.read_parquet(output_file)
        combined_df = pl.concat([existing_df, new_df], how="vertical_relaxed")
        combined_df.write_parquet(output_file, compression="zstd")

        # æ˜¾å¼é‡Šæ”¾å†…å­˜
        del existing_df
        del new_df
        del combined_df
        gc.collect()
    except Exception as e:
        print(f"Warning: Error appending to parquet: {e}")
        # å¤‡ä»½æ–¹æ¡ˆï¼šå†™å…¥ä¸´æ—¶æ–‡ä»¶
        temp_file = f"{output_file}.tmp"
        new_df.write_parquet(temp_file, compression="zstd")
        print(f"Written to temporary file: {temp_file}")
        del new_df
        gc.collect()

def process_single_problem(item: dict, submitter_instance=None) -> dict | None:
    """
    å¤„ç†å•ä¸ªé—®é¢˜ï¼Œè¿”å›åŒ…å«æ‰€æœ‰é€šè¿‡æµ‹è¯•çš„é”™è¯¯ä»£ç åˆ—è¡¨çš„å•æ¡è®°å½•
    ç”¨äºå¹¶è¡Œå¤„ç†

    Returns:
        None æˆ– ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«problem_idå’Œæ‰€æœ‰é€šè¿‡çš„incorrect_codesåˆ—è¡¨
    """
    if submitter_instance is None:
        submitter_instance = LocalCodeSubmitter()

    problem_id = item.get("id")

    try:
        print(f"\n[Processing] {problem_id}")

        # æ£€æŸ¥ checker
        checker_code = item.get("checker")
        if not can_compile_checker(checker_code):
            print(f"  âœ— {problem_id}: checker cannot compile")
            return None

        # è¿‡æ»¤ C++ ä»£ç 
        incorrect_submissions = item.get("incorrect_submissions", [])
        if not incorrect_submissions:
            print(f"  âœ— {problem_id}: no incorrect submissions")
            return None

        cpp_submissions = [
            submission['code']
            for submission in incorrect_submissions
            if submission.get("language") == 'cpp'
        ]

        if len(cpp_submissions) == 0:
            print(f"  âœ— {problem_id}: no C++ submissions")
            return None

        print(f"  {problem_id}: Found {len(cpp_submissions)} C++ incorrect submissions")

        # è·å–é¢˜ç›®æè¿°
        description = item.get("description", "")

        # æ„é€ hack_url
        contest_id, problem_idx = problem_id.split("_")[0], problem_id.split("_")[1]
        hack_url = f"https://codeforces.com/contest/{contest_id}/hacks?verdictName=CHALLENGE_SUCCESSFUL&chosenProblemIndex={problem_idx}"

        # éªŒè¯correct code
        print(f"  {problem_id}: Verifying correct submissions...")
        correct_submissions = item.get("correct_submissions", [])
        correct_code = get_first_correct_cpp_code(correct_submissions, item, submitter_instance)

        # æµ‹è¯•incorrect submissions
        print(f"  {problem_id}: Testing incorrect submissions...")
        incorrect_submissions_result = submitter_instance.batch_submit_code(
            problem_id,
            cpp_submissions,
            item,
            all_judge=False,
            original_result="incorrect"
        )

        # æ”¶é›†æ‰€æœ‰é€šè¿‡çš„codes
        passed_codes = []
        if "passed_submissions" in incorrect_submissions_result:
            for passed_sub in incorrect_submissions_result["passed_submissions"]:
                passed_codes.append(passed_sub["code"])
            print(f"  âœ“ {problem_id}: {len(passed_codes)} passed incorrect submission(s)")
        else:
            print(f"  â„¹ {problem_id}: No incorrect submissions passed")
            return None

        # å¦‚æœæœ‰é€šè¿‡çš„codesï¼Œè¿”å›å•æ¡è®°å½•
        if passed_codes:
            return {
                "id": problem_id,
                "incorrect_codes": passed_codes,  # åˆ—è¡¨å½¢å¼
                "correct_code": correct_code,
                "description": description,
                "checker": item.get("checker"),
                "test_cases": item.get("test_cases"),
                "hack_url": hack_url
            }
        else:
            return None

    except Exception as e:
        print(f"  âœ— Error processing {problem_id}: {e}")
        return None

# åˆå§‹åŒ–
print("="*80)
print("Initializing...")
print("="*80)

# æ˜¾ç¤ºç³»ç»Ÿå†…å­˜ä¿¡æ¯
total_memory_gb = psutil.virtual_memory().total / (1024**3)
available_memory_gb = psutil.virtual_memory().available / (1024**3)
print(f"System Memory: {total_memory_gb:.1f} GB total, {available_memory_gb:.1f} GB available")
print(f"Initial process memory: {get_memory_usage_mb():.1f} MB")
print(f"Memory warning threshold: {MEMORY_WARNING_THRESHOLD_MB} MB")

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯èƒ½ç”±ä¹‹å‰ä¸­æ–­çš„è¿è¡Œç•™ä¸‹çš„ï¼‰
def cleanup_temp_files():
    """æ¸…ç†å½“å‰ç›®å½•å’Œ/tmpä¸‹çš„ä¸´æ—¶ç¼–è¯‘æ–‡ä»¶"""
    import glob
    cleaned = 0

    # æ¸…ç†å½“å‰ç›®å½•çš„ä¸´æ—¶æ–‡ä»¶
    patterns = [
        "checker_test_*.cpp",
        "checker_test_*",
        "correct_quick_*.cpp",
        "correct_quick_*",
        "*.out",
        "case_*.txt"
    ]

    for pattern in patterns:
        for file in glob.glob(pattern):
            try:
                if os.path.isfile(file):
                    os.remove(file)
                    cleaned += 1
            except Exception:
                pass

    if cleaned > 0:
        print(f"âœ“ Cleaned up {cleaned} temporary files")

cleanup_temp_files()

# åŠ è½½ç¼“å­˜å’Œæ–­ç‚¹
load_checker_cache()
checkpoint = load_checkpoint()
processed_problems = set(checkpoint.get("processed_problems", []))
total_saved = checkpoint.get("total_saved", 0)

print(f"âœ“ Checkpoint loaded: {len(processed_problems)} problems already processed")

total_saved = 0

for dataset in os.listdir(dataset_path):
    path = os.path.join(dataset_path, dataset)
    if not path.endswith("parquet"): continue

    # ä¸ºæ¯ä¸ªè¾“å…¥parquetåˆ›å»ºå¯¹åº”çš„è¾“å‡ºæ–‡ä»¶
    dataset_name = os.path.splitext(dataset)[0]  # å»æ‰.parquetåç¼€
    output_file = os.path.join(output_dir, f"{dataset_name}_passed_incorrect.parquet")

    print(f"\n{'='*80}")
    print(f"Processing {dataset} -> {os.path.basename(output_file)}")
    print(f"{'='*80}")

    try:
        lazy_df = pl.scan_parquet(path)

        # åªé€‰æ‹©éœ€è¦çš„åˆ—ï¼Œå‡å°‘å†…å­˜å ç”¨
        selected_columns = [
            "id",
            "test_cases",
            "incorrect_submissions",
            "correct_submissions",  # æ·»åŠ æ­£ç¡®æäº¤
            "description",  # æ·»åŠ é¢˜ç›®æè¿°
            "true_positive_rate",
            "true_negative_rate",
            "checker"
        ]
        lazy_df = lazy_df.select(selected_columns)

        # ä¼˜åŒ–ï¼šæ·»åŠ æ›´å¤šè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡ä¸éœ€è¦å¤„ç†çš„æ•°æ®
        lazy_df = lazy_df.filter(
            pl.col("id").str.contains("_") &
            (pl.col("test_cases").list.len() > 0) &
            (pl.col("true_negative_rate") != 1.0) &
            (pl.col("incorrect_submissions").list.len() > 0) &  # ä¼˜åŒ–ï¼šå¿…é¡»æœ‰incorrect_submissions
            (pl.col("correct_submissions").list.len() > 0)  # ä¼˜åŒ–ï¼šå¿…é¡»æœ‰correct_submissions
        )

        # è·å–æ€»è¡Œæ•°ï¼Œä½¿ç”¨try-exceptå¤„ç†å¯èƒ½çš„parqueté”™è¯¯
        try:
            total_rows = lazy_df.select(pl.len()).collect().item()
            print(f"Total rows to process: {total_rows}")
        except Exception as e:
            print(f"âš  Error getting row count with lazy scan: {e}")
            print(f"  Falling back to eager loading...")
            # å›é€€æ–¹æ¡ˆï¼šç›´æ¥è¯»å–å¹¶è®¡æ•°
            try:
                df = pl.read_parquet(path, columns=selected_columns)
                df = df.filter(
                    pl.col("id").str.contains("_") &
                    (pl.col("test_cases").list.len() > 0) &
                    (pl.col("true_negative_rate") != 1.0) &
                    (pl.col("incorrect_submissions").list.len() > 0) &
                    (pl.col("correct_submissions").list.len() > 0)
                )
                total_rows = len(df)
                print(f"Total rows to process: {total_rows}")
                # ä½¿ç”¨eager loaded dfï¼Œä¸å†ä½¿ç”¨lazy_df
                lazy_df = None
            except Exception as e2:
                print(f"âœ— Error loading {dataset}: {e2}")
                print(f"  Skipping this dataset...")
                continue

    except Exception as e:
        print(f"âœ— Error opening {dataset}: {e}")
        print(f"  Skipping this dataset...")
        continue

    for offset in range(0, total_rows, BATCH_SIZE):
        try:
            # å¦‚æœä½¿ç”¨lazy_df
            if lazy_df is not None:
                batch_df = lazy_df.slice(offset, BATCH_SIZE).collect()
            else:
                # å¦‚æœä½¿ç”¨eager loaded df
                batch_df = df.slice(offset, BATCH_SIZE)

        except Exception as e:
            print(f"âœ— Error loading batch at offset {offset}: {e}")
            print(f"  Skipping this batch...")
            continue

        batch_num = offset//BATCH_SIZE + 1
        total_batches = (total_rows + BATCH_SIZE - 1)//BATCH_SIZE
        print(f"\n{'='*80}")
        print(f"Processing batch {batch_num}/{total_batches} (problems {offset+1}-{min(offset+BATCH_SIZE, total_rows)})")
        print(f"{'='*80}")

        # æ”¶é›†éœ€è¦å¤„ç†çš„é—®é¢˜ï¼ˆè·³è¿‡å·²å¤„ç†çš„ï¼‰
        items_to_process = []
        for item in batch_df.iter_rows(named=True):
            problem_id = item.get("id")
            if problem_id in processed_problems:
                print(f"  â­ Skipping {problem_id} (already processed)")
                continue
            items_to_process.append(item)

        if not items_to_process:
            print(f"  â„¹ All problems in this batch already processed")
            del batch_df
            gc.collect()
            continue

        print(f"  Processing {len(items_to_process)} problems in this batch")

        # å¹¶è¡Œæˆ–é¡ºåºå¤„ç†
        if USE_MULTIPROCESSING and len(items_to_process) > 1:
            print(f"  Using parallel processing with {NUM_WORKERS} workers")
            with Pool(NUM_WORKERS) as pool:
                batch_results = pool.map(process_single_problem, items_to_process)
        else:
            print(f"  Using sequential processing")
            batch_results = [process_single_problem(item, submitter) for item in items_to_process]

        # æ”¶é›†ç»“æœ
        batch_records_to_save = []
        for result, item in zip(batch_results, items_to_process):
            problem_id = item.get("id")

            if result:  # resultæ˜¯å­—å…¸æˆ–None
                num_codes = len(result.get("incorrect_codes", []))
                print(f"  âœ“ {problem_id}: Found {num_codes} passed incorrect code(s)")
                batch_records_to_save.append(result)
            else:
                print(f"  â„¹ {problem_id}: No incorrect submissions passed the tests")

            # æ ‡è®°ä¸ºå·²å¤„ç†
            processed_problems.add(problem_id)

        # æ‰¹æ¬¡å®Œæˆåä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰ç»“æœ
        if batch_records_to_save:
            append_to_parquet(batch_records_to_save, output_file)
            total_saved += len(batch_records_to_save)
            print(f"  ğŸ’¾ [Saved] {len(batch_records_to_save)} problems in this batch (Total: {total_saved} problems)")

        # ä¿å­˜checkpoint
        save_checkpoint(list(processed_problems), total_saved)
        print(f"  ğŸ’¾ [Checkpoint] Saved progress: {len(processed_problems)} problems processed")

        # å†…å­˜ç®¡ç†ï¼šå¦‚æœprocessed_problemså¤ªå¤§ï¼Œå®šæœŸæŒä¹…åŒ–å¹¶æ¸…ç©ºéƒ¨åˆ†å†…å­˜
        if len(processed_problems) > MAX_PROCESSED_PROBLEMS_IN_MEMORY:
            print(f"  âš  processed_problems size: {len(processed_problems)}, flushing to checkpoint...")
            save_checkpoint(list(processed_problems), total_saved)
            # æ³¨æ„ï¼šä¸æ¸…ç©ºprocessed_problemsï¼Œå› ä¸ºéœ€è¦ç”¨æ¥åˆ¤æ–­æ˜¯å¦å·²å¤„ç†

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        mem_mb = check_memory_usage()
        print(f"  ğŸ“Š Memory usage: {mem_mb:.1f} MB")

        del batch_df
        del batch_results  # æ˜¾å¼åˆ é™¤ç»“æœåˆ—è¡¨
        del items_to_process  # æ˜¾å¼åˆ é™¤å¾…å¤„ç†åˆ—è¡¨
        gc.collect()
        print(f"\n  Batch {batch_num} completed. Total saved so far: {total_saved}")

    # æ¸…ç†DataFrame
    if lazy_df is not None:
        del lazy_df
    else:
        del df
    gc.collect()
    print(f"\nâœ“ Completed processing {dataset}\n")

# ä¿å­˜æœ€ç»ˆçŠ¶æ€
save_checker_cache()
save_checkpoint(list(processed_problems), total_saved)

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
cleanup_temp_files()

# æœ€ç»ˆå†…å­˜æŠ¥å‘Š
final_mem_mb = get_memory_usage_mb()
print(f"\n{'='*80}")
print(f"âœ… Finished!")
print(f"{'='*80}")
print(f"Total saved: {total_saved} problems with passed incorrect submissions")
print(f"Total problems processed: {len(processed_problems)}")
print(f"Output directory: {output_dir}")

# åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶
output_files = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]
print(f"Generated {len(output_files)} output files:")
total_size = 0
for output_file in sorted(output_files):
    file_path = os.path.join(output_dir, output_file)
    if os.path.exists(file_path):
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        total_size += file_size
        print(f"  - {output_file}: {file_size:.2f} MB")
print(f"Total output size: {total_size:.2f} MB")

print(f"Checker cache entries: {len(checker_cache)}")
print(f"Final memory usage: {final_mem_mb:.1f} MB")
print(f"{'='*80}")

