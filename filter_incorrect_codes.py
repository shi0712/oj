"""
è¿‡æ»¤å¤šç‰ˆæœ¬æµ‹è¯•è„šæœ¬
è¾“å…¥: *_passed_incorrect.parquet æ–‡ä»¶
æ ¼å¼:
  - id: é¢˜ç›®ID (å¦‚ "755_A")
  - incorrect_codes: List[str] - å¾…è¿‡æ»¤çš„ä»£ç åˆ—è¡¨
  - correct_code: str - æ­£ç¡®ä»£ç 
  - description: str - é¢˜ç›®æè¿°
  - checker: str - SPJä»£ç ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
  - test_cases: List[{input, output}] - æµ‹è¯•æ•°æ®
  - hack_url: str

å¤„ç†æµç¨‹:
1. å¯¹æ¯ä¸ªé¢˜ç›®ï¼Œå…ˆä¸Šä¼ é¢˜ç›®(test_cases, checker)åˆ°OJ
2. å¯¹æ¯ä¸ªincorrect_codeï¼Œæµ‹è¯•c++14 -> c++17 -> c++20 -> c++23
3. å¦‚æœä»»æ„ç‰ˆæœ¬å¤±è´¥ï¼ˆéCEï¼‰ï¼Œä»åˆ—è¡¨ä¸­ç§»é™¤è¯¥ä»£ç 
4. è¾“å‡ºè¿‡æ»¤åçš„parquetæ–‡ä»¶
"""
import os
import polars as pl
from remote_submitter import RemoteOJSubmitter
import aiohttp
import asyncio
import gc
import json
import zipfile
import tempfile
from typing import List
from tqdm import tqdm

# è¾“å…¥ç›®å½•ï¼ˆåŒ…å« *_passed_incorrect.parquet æ–‡ä»¶ï¼‰
input_dir = "../dataset"

# è¾“å‡ºç›®å½•
output_dir = "./output_filtered"

# é”™è¯¯ä»£ç ä¿å­˜ç›®å½•
failed_codes_dir = "./failed_codes"

# checkpointæ–‡ä»¶
checkpoint_file = "checkpoint_filter.json"

# OJæœåŠ¡å™¨åœ°å€
OJ_BASE_URL = "http://localhost:8000"

# åˆ›å»ºè¾“å‡ºç›®å½•
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"âœ“ Created output directory: {output_dir}")

# åˆ›å»ºé”™è¯¯ä»£ç ä¿å­˜ç›®å½•
if not os.path.exists(failed_codes_dir):
    os.makedirs(failed_codes_dir)
    print(f"âœ“ Created failed codes directory: {failed_codes_dir}")

# åˆå§‹åŒ–è¿œç¨‹æäº¤å™¨
submitter = RemoteOJSubmitter(
    base_url=OJ_BASE_URL,
    max_workers=8
)

# æ‰€æœ‰C++ç‰ˆæœ¬ï¼ˆæŒ‰é¡ºåºæµ‹è¯•ï¼‰
CPP_VERSIONS = ["c++17"]


def load_checkpoint():
    """åŠ è½½æ–­ç‚¹ä¿¡æ¯"""
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
            print(f"âœ“ Resuming from checkpoint")
            return checkpoint
        except Exception as e:
            print(f"âš  Failed to load checkpoint: {e}")
    return {"processed_problems": []}


def save_checkpoint(processed_problems: list):
    """ä¿å­˜æ–­ç‚¹ä¿¡æ¯"""
    try:
        checkpoint = {
            "processed_problems": processed_problems,
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)
    except Exception as e:
        print(f"âš  Failed to save checkpoint: {e}")


async def check_problem_exists(problem_id: str) -> bool:
    """æ£€æŸ¥é¢˜ç›®æ˜¯å¦å·²å­˜åœ¨"""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(f"{OJ_BASE_URL}/api/problems/{problem_id}") as resp:
                return resp.status == 200
        except Exception:
            return False


async def upload_problem(problem_id: str, test_cases: list, checker: str = None,
                         time_limit: int = 2000, memory_limit: int = 256):
    """
    ä¸Šä¼ é¢˜ç›®åˆ°OJæœåŠ¡å™¨ï¼ˆå¦‚æœé¢˜ç›®å·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰

    Args:
        problem_id: é¢˜ç›®ID
        test_cases: æµ‹è¯•æ•°æ®åˆ—è¡¨ [{input, output}, ...]
        checker: SPJä»£ç ï¼ˆå¯é€‰ï¼‰
        time_limit: æ—¶é—´é™åˆ¶(ms)
        memory_limit: å†…å­˜é™åˆ¶(MB)

    Returns:
        True if successful or already exists, False otherwise
    """
    # æ£€æŸ¥é¢˜ç›®æ˜¯å¦å·²å­˜åœ¨
    if await check_problem_exists(problem_id):
        print(f"    âœ“ Problem already exists, skipping upload")
        return True

    # åˆ›å»ºä¸´æ—¶zipæ–‡ä»¶
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, "testcases.zip")

        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæ‰“åŒ…è¿›åº¦
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for idx, tc in tqdm(enumerate(test_cases, 1), total=len(test_cases),
                               desc="    Packing testcases", leave=False):
                input_data = tc.get("input", "")
                output_data = tc.get("output", "")
                zf.writestr(f"{idx}.in", input_data)
                zf.writestr(f"{idx}.out", output_data)

        # ä¸Šä¼ åˆ°OJ
        async with aiohttp.ClientSession() as session:
            data = aiohttp.FormData()
            data.add_field('problem_id', problem_id)
            data.add_field('time_limit', str(time_limit))
            data.add_field('memory_limit', str(memory_limit))

            with open(zip_path, 'rb') as f:
                data.add_field('testcases', f, filename='testcases.zip',
                              content_type='application/zip')

                if checker and checker.strip():
                    # Send checker as file content, not string
                    data.add_field('checker', checker.encode('utf-8'),
                                  filename='checker.cpp',
                                  content_type='text/plain')

                print(f"    Uploading to OJ server...")
                async with session.post(f"{OJ_BASE_URL}/api/problems", data=data) as resp:
                    result = await resp.json()
                    if resp.status == 200:
                        return True
                    else:
                        print(f"    âš  Upload failed: {result}")
                        return False


async def test_code_all_versions_async(problem_id: str, code: str, max_retries: int = 2) -> dict:
    """
    å¼‚æ­¥æµ‹è¯•ä»£ç åœ¨æ‰€æœ‰C++ç‰ˆæœ¬ä¸Šçš„è¡¨ç°
    ä»c++14åˆ°c++23é¡ºåºæµ‹è¯•ï¼Œä¸€æ—¦æŸä¸ªç‰ˆæœ¬å¤±è´¥ï¼ˆéCEï¼‰å°±ç«‹å³åœæ­¢
    System Error ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆTLEé‡è¯•å·²åœ¨OJå†…éƒ¨å¤„ç†ï¼‰

    Returns:
        {
            "all_passed": bool,
            "stopped_at": str or None,
            "fail_verdict": str or None,
            "failed_test": int or None,
            "version_results": dict
        }
    """
    version_results = {}

    for version in CPP_VERSIONS:
        # System Error è‡ªåŠ¨é‡è¯•
        for retry in range(max_retries):
            result = await submitter.submit_code_async(
                problem_id=problem_id,
                code=code,
                language=version
            )

            verdict = result.get("verdict", "System Error")
            passed = result.get("passed", False)
            failed_test = result.get("failed_test")

            # åªå¯¹ System Error é‡è¯•
            if verdict != "System Error":
                break

            if retry < max_retries - 1:
                await asyncio.sleep(0.5)

        version_results[version] = {
            "verdict": verdict,
            "passed": passed,
            "failed_test": failed_test
        }

        # å¦‚æœä¸æ˜¯CEï¼Œæ£€æŸ¥æ˜¯å¦é€šè¿‡
        if verdict != "Compile Error":
            if not passed:
                # é‡åˆ°å¤±è´¥ï¼Œç«‹å³åœæ­¢
                return {
                    "all_passed": False,
                    "stopped_at": version,
                    "fail_verdict": verdict,
                    "failed_test": failed_test,
                    "version_results": version_results
                }

    # æ‰€æœ‰ç‰ˆæœ¬éƒ½é€šè¿‡ï¼ˆæˆ–CEï¼‰
    return {
        "all_passed": True,
        "stopped_at": None,
        "fail_verdict": None,
        "failed_test": None,
        "version_results": version_results
    }


def test_code_all_versions(problem_id: str, code: str) -> dict:
    """
    æµ‹è¯•ä»£ç åœ¨æ‰€æœ‰C++ç‰ˆæœ¬ä¸Šçš„è¡¨ç°ï¼ˆåŒæ­¥åŒ…è£…ï¼‰
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(test_code_all_versions_async(problem_id, code))
    finally:
        loop.close()


def save_failed_code(problem_id: str, code: str, index: int, fail_info: dict):
    """
    ä¿å­˜å¤±è´¥çš„ä»£ç åˆ°æ–‡ä»¶

    Args:
        problem_id: é¢˜ç›®ID
        code: ä»£ç å†…å®¹
        index: ä»£ç åºå·
        fail_info: å¤±è´¥ä¿¡æ¯ {stopped_at, fail_verdict, failed_test}
    """
    # åˆ›å»ºé¢˜ç›®ç›®å½•
    problem_dir = os.path.join(failed_codes_dir, problem_id)
    if not os.path.exists(problem_dir):
        os.makedirs(problem_dir)

    # ä¿å­˜ä»£ç æ–‡ä»¶
    code_file = os.path.join(problem_dir, f"{index}.cpp")
    with open(code_file, 'w', encoding='utf-8') as f:
        f.write(code)

    # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°åŒåçš„txtæ–‡ä»¶
    info_file = os.path.join(problem_dir, f"{index}.txt")
    with open(info_file, 'w', encoding='utf-8') as f:
        f.write(f"Failed at: {fail_info['stopped_at']}\n")
        f.write(f"Verdict: {fail_info['fail_verdict']}\n")
        if fail_info['failed_test'] is not None:
            f.write(f"Failed test: {fail_info['failed_test']}\n")


async def filter_incorrect_codes_async(problem_id: str, incorrect_codes: List[str], max_concurrent: int = 2) -> List[str]:
    """
    å¼‚æ­¥å¹¶å‘è¿‡æ»¤ incorrect_codes åˆ—è¡¨
    å¤±è´¥çš„ä»£ç ä¼šä¿å­˜åˆ° failed_codes_dir/{problem_id}/ ç›®å½•

    Args:
        problem_id: é¢˜ç›®ID
        incorrect_codes: å¾…æµ‹è¯•çš„ä»£ç åˆ—è¡¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤2ï¼Œé¿å…CPUç«äº‰ï¼‰
    """
    filtered_codes = []
    failed_code_count = 0
    semaphore = asyncio.Semaphore(max_concurrent)

    async def test_single_code(idx: int, code: str):
        nonlocal failed_code_count
        async with semaphore:
            result = await test_code_all_versions_async(problem_id, code)

            if result["all_passed"]:
                filtered_codes.append(code)
                tqdm.write(f"    [{idx+1}/{len(incorrect_codes)}] âœ“ Passed all versions")
                return True
            else:
                stopped_at = result.get("stopped_at")
                fail_verdict = result.get("fail_verdict")
                failed_test = result.get("failed_test")

                # ä¿å­˜å¤±è´¥çš„ä»£ç 
                failed_code_count += 1
                save_failed_code(
                    problem_id=problem_id,
                    code=code,
                    index=failed_code_count,
                    fail_info={
                        'stopped_at': stopped_at,
                        'fail_verdict': fail_verdict,
                        'failed_test': failed_test
                    }
                )

                # è¾“å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
                error_info = f"{fail_verdict}"
                if failed_test is not None:
                    error_info += f" on test {failed_test}"

                tqdm.write(f"    [{idx+1}/{len(incorrect_codes)}] âœ— Failed at {stopped_at}: {error_info} -> Saved as {failed_code_count}.cpp")
                return False

    # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè¿‡æ»¤è¿›åº¦
    tasks = []
    with tqdm(total=len(incorrect_codes), desc="    Testing codes", leave=False) as pbar:
        for idx, code in enumerate(incorrect_codes):
            task = asyncio.create_task(test_single_code(idx, code))
            task.add_done_callback(lambda _: pbar.update(1))
            tasks.append(task)

        await asyncio.gather(*tasks)

    return filtered_codes


def filter_incorrect_codes(problem_id: str, incorrect_codes: List[str]) -> List[str]:
    """
    è¿‡æ»¤ incorrect_codes åˆ—è¡¨ï¼ˆåŒæ­¥åŒ…è£…ï¼‰
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(
            filter_incorrect_codes_async(problem_id, incorrect_codes, max_concurrent=2)
        )
    finally:
        loop.close()


def is_code_failed(problem_id: str, code: str) -> bool:
    """æ£€æŸ¥æŸä¸ªä»£ç æ˜¯å¦åœ¨failed_codesç›®å½•ä¸­
    é€ä¸ªæ–‡ä»¶è¯»å–å¹¶æ¯”è¾ƒï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å¤±è´¥ä»£ç åˆ°å†…å­˜

    Args:
        problem_id: é¢˜ç›®ID
        code: å¾…æ£€æŸ¥çš„ä»£ç å†…å®¹

    Returns:
        True if code is in failed_codes, False otherwise
    """
    problem_dir = os.path.join(failed_codes_dir, problem_id)
    if not os.path.exists(problem_dir):
        return False

    for filename in os.listdir(problem_dir):
        if filename.endswith('.cpp'):
            filepath = os.path.join(problem_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    failed_code = f.read()
                    if failed_code == code:
                        return True
            except Exception as e:
                print(f"    âš  Failed to read {filepath}: {e}")

    return False


def process_parquet_file(input_file: str, output_file: str, processed_problems: set):
    """
    å¤„ç†å•ä¸ª parquet æ–‡ä»¶
    """
    print(f"\n{'='*80}")
    print(f"Processing: {os.path.basename(input_file)}")
    print(f"{'='*80}")

    # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è·³è¿‡æ•´ä¸ªæ–‡ä»¶
    if os.path.exists(output_file):
        print(f"  â­ Output file already exists, skipping entire file")
        return

    # è¯»å– parquet æ–‡ä»¶ï¼ˆç¦ç”¨å†…å­˜æ˜ å°„é¿å… Windows æ–‡ä»¶é”å®šé—®é¢˜ï¼‰
    try:
        df = pl.read_parquet(input_file, use_pyarrow=False)
        print(f"Loaded {len(df)} rows")
        print(f"Columns: {df.columns}")
    except Exception as e:
        print(f"âœ— Error loading file: {e}")
        return

    # å¤„ç†æ¯ä¸€è¡Œ
    filtered_records = []

    try:
        for row in df.iter_rows(named=True):
            problem_id = row.get("id")

            if problem_id in processed_problems:
                # ä»failed_codesæ¢å¤filtered_codesï¼ˆé€ä¸ªæ¯”è¾ƒï¼Œé¿å…åŠ è½½æ‰€æœ‰ä»£ç åˆ°å†…å­˜ï¼‰
                print(f"  â­ {problem_id} already processed, recovering from failed_codes...")
                incorrect_codes = row.get("incorrect_codes", [])
                if not incorrect_codes:
                    print(f"    â„¹ No incorrect codes, skipping")
                    continue

                # é€ä¸ªæ£€æŸ¥ä»£ç æ˜¯å¦å¤±è´¥ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å¤±è´¥ä»£ç 
                filtered_codes = []
                for code in incorrect_codes:
                    if not is_code_failed(problem_id, code):
                        filtered_codes.append(code)
                    # ç«‹å³é‡Šæ”¾codeï¼Œé¿å…å†…å­˜ç´¯ç§¯
                    del code

                if filtered_codes:
                    # æ¢å¤è®°å½•åˆ°è¾“å‡º
                    new_record = dict(row)
                    new_record["incorrect_codes"] = filtered_codes
                    filtered_records.append(new_record)
                    print(f"    âœ“ Recovered: {len(filtered_codes)}/{len(incorrect_codes)} codes (kept in dataset)")
                else:
                    print(f"    â„¹ All codes filtered out (removed from dataset)")

                # æ¸…ç†ä¸´æ—¶å˜é‡
                del incorrect_codes, filtered_codes
                continue

            incorrect_codes = row.get("incorrect_codes", [])
            if not incorrect_codes:
                print(f"  â„¹ {problem_id}: No incorrect codes, skipping")
                processed_problems.add(problem_id)
                continue

            test_cases = row.get("test_cases", [])
            checker = row.get("checker")
            time_limit = row.get("time_limit")
            memory_limit = row.get("memory_limit")

            print(f"\n  [Processing] {problem_id}: {len(incorrect_codes)} codes, {len(test_cases)} test cases")

            # 1. ä¸Šä¼ é¢˜ç›®åˆ°OJ
            print(f"    Uploading problem to OJ...")
            upload_success = asyncio.run(upload_problem(
                problem_id=problem_id,
                test_cases=test_cases,
                checker=checker,
                time_limit=time_limit,
                memory_limit=memory_limit
            ))

            if not upload_success:
                print(f"    âš  Failed to upload problem, skipping")
                processed_problems.add(problem_id)
                # æ¸…ç†å¤§å¯¹è±¡
                del test_cases, incorrect_codes
                continue

            print(f"    âœ“ Problem uploaded")

            # 2. è¿‡æ»¤ä»£ç 
            print(f"    Filtering codes...")
            original_count = len(incorrect_codes)
            filtered_codes = filter_incorrect_codes(problem_id, incorrect_codes)

            # æ¸…ç†ä¸å†éœ€è¦çš„å¤§å¯¹è±¡
            del test_cases, incorrect_codes
            gc.collect()

            # æ ‡è®°å·²å¤„ç†
            processed_problems.add(problem_id)

            if filtered_codes:
                # åˆ›å»ºæ–°è®°å½•ï¼Œä¿ç•™åŸæœ‰å­—æ®µï¼Œæ›´æ–° incorrect_codes
                new_record = dict(row)
                new_record["incorrect_codes"] = filtered_codes
                filtered_records.append(new_record)
                print(f"  âœ“ {problem_id}: {len(filtered_codes)}/{original_count} codes passed all versions")
            else:
                print(f"  â„¹ {problem_id}: All codes filtered out")

            # æ¸…ç†ä¸´æ—¶å˜é‡
            del filtered_codes

            # å®šæœŸä¿å­˜checkpoint
            save_checkpoint(list(processed_problems))

    finally:
        # æ¸…ç†DataFrame
        del df
        gc.collect()

    # ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰è®°å½•
    if filtered_records:
        print(f"\nğŸ’¾ Writing {len(filtered_records)} records to output file...")
        new_df = pl.DataFrame(filtered_records)
        new_df.write_parquet(output_file, compression="zstd")
        del new_df
        print(f"  âœ“ Saved to {output_file}")
    else:
        print(f"\n  â„¹ No records to save")

    gc.collect()
    print(f"\nâœ“ Finished processing {os.path.basename(input_file)}")


# ä¸»ç¨‹åº
if __name__ == "__main__":
    import sys

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # å•æ–‡ä»¶æ¨¡å¼ï¼špython filter_incorrect_codes.py <filename>
        target_filename = sys.argv[1]
        print("="*80)
        print(f"Filter Incorrect Codes - Single File Mode")
        print("="*80)
        print(f"Target file: {target_filename}")
        print(f"Input directory: {input_dir}")
        print(f"Output directory: {output_dir}")
        print(f"C++ versions: {CPP_VERSIONS}")

        # åŠ è½½æ–­ç‚¹
        checkpoint = load_checkpoint()
        processed_problems = set(checkpoint.get("processed_problems", []))

        # å¤„ç†å•ä¸ªæ–‡ä»¶
        input_file = os.path.join(input_dir, target_filename)
        if not os.path.exists(input_file):
            print(f"âœ— Error: File not found: {input_file}")
            sys.exit(1)

        output_filename = target_filename.replace("_passed_incorrect.parquet", "_filtered.parquet")
        output_file = os.path.join(output_dir, output_filename)

        process_parquet_file(input_file, output_file, processed_problems)

        # ä¿å­˜checkpoint
        save_checkpoint(list(processed_problems))

        print(f"\n{'='*80}")
        print(f"âœ… Finished processing {target_filename}!")
        print(f"{'='*80}")

    else:
        # æ‰¹é‡æ¨¡å¼ï¼špython filter_incorrect_codes.py
        print("="*80)
        print("Filter Incorrect Codes - Batch Mode")
        print("="*80)
        print(f"Input directory: {input_dir}")
        print(f"Output directory: {output_dir}")
        print(f"C++ versions: {CPP_VERSIONS}")

        # åŠ è½½æ–­ç‚¹
        checkpoint = load_checkpoint()
        processed_problems = set(checkpoint.get("processed_problems", []))
        print(f"âœ“ Already processed: {len(processed_problems)} problems")

        # éå†è¾“å…¥ç›®å½•
        parquet_files = [f for f in os.listdir(input_dir) if f.endswith("_passed_incorrect.parquet")]
        print(f"Found {len(parquet_files)} parquet files")

        for filename in sorted(parquet_files):
            input_file = os.path.join(input_dir, filename)

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_filename = filename.replace("_passed_incorrect.parquet", "_filtered.parquet")
            output_file = os.path.join(output_dir, output_filename)

            process_parquet_file(input_file, output_file, processed_problems)

        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        save_checkpoint(list(processed_problems))

        # æœ€ç»ˆæŠ¥å‘Š
        print(f"\n{'='*80}")
        print(f"âœ… Finished!")
        print(f"{'='*80}")
        print(f"Total problems processed: {len(processed_problems)}")
        print(f"Output directory: {output_dir}")

        # åˆ—å‡ºè¾“å‡ºæ–‡ä»¶
        if os.path.exists(output_dir):
            output_files = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]
            print(f"Generated {len(output_files)} output files:")
            total_size = 0
            for output_file in sorted(output_files):
                file_path = os.path.join(output_dir, output_file)
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path) / (1024 * 1024)
                    total_size += file_size
                    print(f"  - {output_file}: {file_size:.2f} MB")
            print(f"Total output size: {total_size:.2f} MB")

        # ç»Ÿè®¡å¤±è´¥ä»£ç 
        if os.path.exists(failed_codes_dir):
            problem_dirs = [d for d in os.listdir(failed_codes_dir)
                            if os.path.isdir(os.path.join(failed_codes_dir, d))]
            total_failed = 0
            for problem_dir in problem_dirs:
                failed_count = len([f for f in os.listdir(os.path.join(failed_codes_dir, problem_dir))
                                   if f.endswith('.cpp')])
                total_failed += failed_count
            print(f"\nFailed codes directory: {failed_codes_dir}")
            print(f"Total failed codes: {total_failed} codes across {len(problem_dirs)} problems")

        print(f"{'='*80}")
