"""
å¤šç‰ˆæœ¬æµ‹è¯•è„šæœ¬ - ä½¿ç”¨RemoteOJSubmitteræµ‹è¯•æ‰€æœ‰C++ç‰ˆæœ¬
å¯¹äºæ¯ä¸ªä»£ç ï¼Œæµ‹è¯•c++14/17/20/23æ‰€æœ‰ç‰ˆæœ¬ï¼Œå¦‚æœä»»æ„ç‰ˆæœ¬å¤±è´¥ï¼ˆæ’é™¤CEï¼‰åˆ™è®¤ä¸ºä»£ç é”™è¯¯
"""
import os
import polars as pl
from remote_submitter import RemoteOJSubmitter
import gc
import json
from typing import Optional
import psutil

dataset_path = "./dataset/ccplus_1x"

# åˆå§‹åŒ–è¿œç¨‹æäº¤å™¨
submitter = RemoteOJSubmitter(
    base_url="http://localhost:8000",
    max_workers=8
)

# è¾“å‡ºç›®å½•
output_dir = "output_passed_incorrect_multiversion"
checkpoint_file = "checkpoint_multiversion.json"

# åˆ›å»ºè¾“å‡ºç›®å½•
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"âœ“ Created output directory: {output_dir}")

# æ‰¹é‡å¤„ç†å¤§å°
BATCH_SIZE = 100

# å†…å­˜ç®¡ç†é…ç½®
MAX_PROCESSED_PROBLEMS_IN_MEMORY = 10000
MEMORY_WARNING_THRESHOLD_MB = 8000

# æ‰€æœ‰C++ç‰ˆæœ¬
CPP_VERSIONS = ["c++14", "c++17", "c++20", "c++23"]

def get_memory_usage_mb():
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def check_memory_usage():
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨ï¼Œå¦‚æœè¶…è¿‡é˜ˆå€¼åˆ™è­¦å‘Š"""
    mem_mb = get_memory_usage_mb()
    if mem_mb > MEMORY_WARNING_THRESHOLD_MB:
        print(f"âš  WARNING: Memory usage is high: {mem_mb:.1f} MB")
    return mem_mb

def load_checkpoint():
    """åŠ è½½æ–­ç‚¹ä¿¡æ¯"""
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
            print(f"âœ“ Resuming from checkpoint: {checkpoint}")
            return checkpoint
        except Exception as e:
            print(f"âš  Failed to load checkpoint: {e}")
    return {"processed_problems": [], "total_saved": 0}

def save_checkpoint(processed_problems: list, total_saved: int):
    """ä¿å­˜æ–­ç‚¹ä¿¡æ¯"""
    try:
        checkpoint = {
            "processed_problems": processed_problems,
            "total_saved": total_saved,
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)
    except Exception as e:
        print(f"âš  Failed to save checkpoint: {e}")

def test_code_all_versions(problem_id: str, code: str, problem_info: dict) -> dict:
    """
    æµ‹è¯•ä»£ç åœ¨æ‰€æœ‰C++ç‰ˆæœ¬ä¸Šçš„è¡¨ç°
    ä»c++14åˆ°c++23é¡ºåºæµ‹è¯•ï¼Œä¸€æ—¦æŸä¸ªç‰ˆæœ¬å¤±è´¥ï¼ˆéCEï¼‰å°±ç«‹å³åœæ­¢

    Returns:
        {
            "all_passed": bool,  # æ‰€æœ‰éCEç‰ˆæœ¬éƒ½é€šè¿‡
            "version_results": {
                "c++14": {"verdict": "...", "passed": bool},
                "c++17": {...},
                ...
            },
            "stopped_at": str  # åœæ­¢çš„ç‰ˆæœ¬ï¼ˆå¦‚æœæå‰åœæ­¢ï¼‰
        }
    """
    version_results = {}
    stopped_at = None

    for version in CPP_VERSIONS:
        result = submitter.submit_code(
            problem_id=problem_id,
            code=code,
            problem_info=problem_info,
            language=version
        )

        verdict = result.get("verdict", "System Error")
        passed = result.get("passed", False)

        version_results[version] = {
            "verdict": verdict,
            "passed": passed,
            "time": result.get("time", 0),
            "memory": result.get("memory", 0),
            "failed_test": result.get("failed_test")
        }

        # å¦‚æœä¸æ˜¯CEï¼Œæ£€æŸ¥æ˜¯å¦é€šè¿‡
        if verdict != "Compile Error":
            if not passed:
                # é‡åˆ°å¤±è´¥ï¼Œç«‹å³åœæ­¢
                stopped_at = version
                return {
                    "all_passed": False,
                    "version_results": version_results,
                    "stopped_at": stopped_at
                }

    # å¦‚æœæ‰€æœ‰ç‰ˆæœ¬éƒ½æ˜¯CEï¼Œè®¤ä¸ºé€šè¿‡ï¼ˆå› ä¸ºæ— æ³•æµ‹è¯•ï¼‰
    # å¦‚æœæ‰€æœ‰éCEç‰ˆæœ¬éƒ½é€šè¿‡ï¼Œä¹Ÿè®¤ä¸ºé€šè¿‡
    all_passed = True

    return {
        "all_passed": all_passed,
        "version_results": version_results,
        "stopped_at": stopped_at
    }

def get_first_correct_cpp_code(correct_submissions: list, problem_item: dict, submitter_instance) -> Optional[str]:
    """
    ä»correct_submissionsä¸­è·å–ç¬¬ä¸€ä¸ªåœ¨æ‰€æœ‰ç‰ˆæœ¬ä¸Šéƒ½ACçš„C++ä»£ç 

    Args:
        correct_submissions: æ­£ç¡®æäº¤åˆ—è¡¨
        problem_item: é¢˜ç›®æ•°æ®
        submitter_instance: RemoteOJSubmitterå®ä¾‹

    Returns:
        ç¬¬ä¸€ä¸ªåœ¨æ‰€æœ‰ç‰ˆæœ¬ä¸Šéƒ½éªŒè¯é€šè¿‡çš„C++ä»£ç ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    if not correct_submissions:
        return None

    cpp_count = 0
    problem_id = problem_item.get("id")
    problem_info = {
        "test_cases": problem_item.get("test_cases"),
        "checker": problem_item.get("checker"),
        "time_limit": problem_item.get("time_limit", 1000),
        "memory_limit": problem_item.get("memory_limit", 256)
    }

    for submission in correct_submissions:
        if submission.get("language") != 'cpp':
            continue

        code = submission.get("code")
        if not code or len(code.strip()) == 0:
            continue

        cpp_count += 1
        print(f"  éªŒè¯ correct submission #{cpp_count}...")

        # æµ‹è¯•æ‰€æœ‰ç‰ˆæœ¬
        multi_result = test_code_all_versions(problem_id, code, problem_info)

        if multi_result["all_passed"]:
            print(f"  âœ“ Found verified correct code (all versions passed, {len(code)} chars)")
            return code
        else:
            # æ˜¾ç¤ºå“ªäº›ç‰ˆæœ¬å¤±è´¥äº†
            failed_versions = [
                v for v, r in multi_result["version_results"].items()
                if not r["passed"] and r["verdict"] != "Compile Error"
            ]
            print(f"  âœ— Failed on versions: {failed_versions}")
            continue

    if cpp_count == 0:
        print(f"  âš  No C++ submissions found")
    else:
        print(f"  âš  No correct submission passed all tests on all versions (tried {cpp_count} submissions)")
    return None

def append_to_parquet(records: list, output_file: str):
    """è¿½åŠ è®°å½•åˆ°parquetæ–‡ä»¶"""
    if not records:
        return

    new_df = pl.DataFrame(records)

    if not os.path.exists(output_file):
        new_df.write_parquet(output_file, compression="zstd")
        del new_df
        gc.collect()
        return

    try:
        existing_df = pl.read_parquet(output_file)
        combined_df = pl.concat([existing_df, new_df], how="vertical_relaxed")
        combined_df.write_parquet(output_file, compression="zstd")

        del existing_df
        del new_df
        del combined_df
        gc.collect()
    except Exception as e:
        print(f"Warning: Error appending to parquet: {e}")
        temp_file = f"{output_file}.tmp"
        new_df.write_parquet(temp_file, compression="zstd")
        print(f"Written to temporary file: {temp_file}")
        del new_df
        gc.collect()

def process_single_problem(item: dict, submitter_instance) -> dict | None:
    """
    å¤„ç†å•ä¸ªé—®é¢˜ï¼Œæµ‹è¯•æ‰€æœ‰C++ç‰ˆæœ¬

    Returns:
        None æˆ–åŒ…å«æ‰€æœ‰é€šè¿‡æµ‹è¯•çš„é”™è¯¯ä»£ç åˆ—è¡¨çš„å­—å…¸
    """
    problem_id = item.get("id")

    try:
        print(f"\n[Processing] {problem_id}")

        # è¿‡æ»¤ C++ ä»£ç 
        incorrect_submissions = item.get("incorrect_submissions", [])
        if not incorrect_submissions:
            print(f"  âœ— {problem_id}: no incorrect submissions")
            return None

        cpp_submissions = [
            submission['code']
            for submission in incorrect_submissions
            if submission.get("language") == 'cpp'
        ]

        if len(cpp_submissions) == 0:
            print(f"  âœ— {problem_id}: no C++ submissions")
            return None

        print(f"  {problem_id}: Found {len(cpp_submissions)} C++ incorrect submissions")

        # è·å–é¢˜ç›®æè¿°
        description = item.get("description", "")

        # æ„é€ hack_url
        contest_id, problem_idx = problem_id.split("_")[0], problem_id.split("_")[1]
        hack_url = f"https://codeforces.com/contest/{contest_id}/hacks?verdictName=CHALLENGE_SUCCESSFUL&chosenProblemIndex={problem_idx}"

        # éªŒè¯correct code
        print(f"  {problem_id}: Verifying correct submissions...")
        correct_submissions = item.get("correct_submissions", [])
        correct_code = get_first_correct_cpp_code(correct_submissions, item, submitter_instance)

        # æ„é€ problem_info
        problem_info = {
            "test_cases": item.get("test_cases"),
            "checker": item.get("checker"),
            "time_limit": item.get("time_limit", 1000),
            "memory_limit": item.get("memory_limit", 256)
        }

        # æµ‹è¯•incorrect submissions
        print(f"  {problem_id}: Testing incorrect submissions on all C++ versions...")
        passed_codes = []

        for idx, code in enumerate(cpp_submissions):
            # æµ‹è¯•æ‰€æœ‰ç‰ˆæœ¬ï¼ˆä¼šåœ¨ç¬¬ä¸€ä¸ªå¤±è´¥ç‰ˆæœ¬åœæ­¢ï¼‰
            multi_result = test_code_all_versions(problem_id, code, problem_info)

            # å¦‚æœæ‰€æœ‰éCEç‰ˆæœ¬éƒ½é€šè¿‡ï¼Œåˆ™è®¤ä¸ºæ˜¯"é”™è¯¯åœ°é€šè¿‡"çš„ä»£ç 
            if multi_result["all_passed"]:
                passed_codes.append({
                    "code": code,
                    "version_results": multi_result["version_results"]
                })
                print(f"    [{idx+1}/{len(cpp_submissions)}] âœ“ Passed on all versions")
            else:
                # æ˜¾ç¤ºåœ¨å“ªä¸ªç‰ˆæœ¬å¤±è´¥
                stopped_at = multi_result.get("stopped_at")
                if stopped_at:
                    print(f"    [{idx+1}/{len(cpp_submissions)}] âœ— Failed at {stopped_at}")
                else:
                    print(f"    [{idx+1}/{len(cpp_submissions)}] âœ— Failed")

        if passed_codes:
            print(f"  âœ“ {problem_id}: {len(passed_codes)} passed on all versions")
            return {
                "id": problem_id,
                "incorrect_codes": [item["code"] for item in passed_codes],
                "correct_code": correct_code,
                "description": description,
                "checker": item.get("checker"),
                "test_cases": item.get("test_cases"),
                "hack_url": hack_url
            }
        else:
            print(f"  â„¹ {problem_id}: No incorrect submissions passed all versions")
            return None

    except Exception as e:
        print(f"  âœ— Error processing {problem_id}: {e}")
        import traceback
        traceback.print_exc()
        return None

# åˆå§‹åŒ–
print("="*80)
print("Initializing Multi-Version Test...")
print("="*80)

# æ˜¾ç¤ºç³»ç»Ÿå†…å­˜ä¿¡æ¯
total_memory_gb = psutil.virtual_memory().total / (1024**3)
available_memory_gb = psutil.virtual_memory().available / (1024**3)
print(f"System Memory: {total_memory_gb:.1f} GB total, {available_memory_gb:.1f} GB available")
print(f"Initial process memory: {get_memory_usage_mb():.1f} MB")
print(f"Testing C++ versions: {CPP_VERSIONS}")

# åŠ è½½æ–­ç‚¹
checkpoint = load_checkpoint()
processed_problems = set(checkpoint.get("processed_problems", []))
total_saved = checkpoint.get("total_saved", 0)

print(f"âœ“ Checkpoint loaded: {len(processed_problems)} problems already processed")

for dataset in os.listdir(dataset_path):
    path = os.path.join(dataset_path, dataset)
    if not path.endswith("parquet"):
        continue

    # ä¸ºæ¯ä¸ªè¾“å…¥parquetåˆ›å»ºå¯¹åº”çš„è¾“å‡ºæ–‡ä»¶
    dataset_name = os.path.splitext(dataset)[0]
    output_file = os.path.join(output_dir, f"{dataset_name}_multiversion_passed.parquet")

    print(f"\n{'='*80}")
    print(f"Processing {dataset} -> {os.path.basename(output_file)}")
    print(f"{'='*80}")

    try:
        lazy_df = pl.scan_parquet(path)

        # åªé€‰æ‹©éœ€è¦çš„åˆ—
        selected_columns = [
            "id",
            "test_cases",
            "incorrect_submissions",
            "correct_submissions",
            "description",
            "true_positive_rate",
            "true_negative_rate",
            "checker",
            "time_limit",
            "memory_limit"
        ]
        lazy_df = lazy_df.select(selected_columns)

        # è¿‡æ»¤æ¡ä»¶
        lazy_df = lazy_df.filter(
            pl.col("id").str.contains("_") &
            (pl.col("test_cases").list.len() > 0) &
            (pl.col("true_negative_rate") != 1.0) &
            (pl.col("incorrect_submissions").list.len() > 0) &
            (pl.col("correct_submissions").list.len() > 0)
        )

        # è·å–æ€»è¡Œæ•°
        try:
            total_rows = lazy_df.select(pl.len()).collect().item()
            print(f"Total rows to process: {total_rows}")
        except Exception as e:
            print(f"âš  Error getting row count: {e}")
            df = pl.read_parquet(path, columns=selected_columns)
            df = df.filter(
                pl.col("id").str.contains("_") &
                (pl.col("test_cases").list.len() > 0) &
                (pl.col("true_negative_rate") != 1.0) &
                (pl.col("incorrect_submissions").list.len() > 0) &
                (pl.col("correct_submissions").list.len() > 0)
            )
            total_rows = len(df)
            print(f"Total rows to process: {total_rows}")
            lazy_df = None

    except Exception as e:
        print(f"âœ— Error opening {dataset}: {e}")
        continue

    for offset in range(0, total_rows, BATCH_SIZE):
        try:
            if lazy_df is not None:
                batch_df = lazy_df.slice(offset, BATCH_SIZE).collect()
            else:
                batch_df = df.slice(offset, BATCH_SIZE)

        except Exception as e:
            print(f"âœ— Error loading batch at offset {offset}: {e}")
            continue

        batch_num = offset // BATCH_SIZE + 1
        total_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"\n{'='*80}")
        print(f"Processing batch {batch_num}/{total_batches} (problems {offset+1}-{min(offset+BATCH_SIZE, total_rows)})")
        print(f"{'='*80}")

        # æ”¶é›†éœ€è¦å¤„ç†çš„é—®é¢˜
        items_to_process = []
        for item in batch_df.iter_rows(named=True):
            problem_id = item.get("id")
            if problem_id in processed_problems:
                print(f"  â­ Skipping {problem_id} (already processed)")
                continue
            items_to_process.append(item)

        if not items_to_process:
            print(f"  â„¹ All problems in this batch already processed")
            del batch_df
            gc.collect()
            continue

        print(f"  Processing {len(items_to_process)} problems in this batch")

        # é¡ºåºå¤„ç†ï¼ˆå› ä¸ºRemoteOJSubmitterå†…éƒ¨å·²ç»æœ‰å¹¶å‘æ§åˆ¶ï¼‰
        batch_results = [process_single_problem(item, submitter) for item in items_to_process]

        # æ”¶é›†ç»“æœ
        batch_records_to_save = []
        for result, item in zip(batch_results, items_to_process):
            problem_id = item.get("id")

            if result:
                num_codes = len(result.get("incorrect_codes", []))
                print(f"  âœ“ {problem_id}: Found {num_codes} code(s) passed all versions")
                batch_records_to_save.append(result)
            else:
                print(f"  â„¹ {problem_id}: No incorrect submissions passed all versions")

            processed_problems.add(problem_id)

        # æ‰¹æ¬¡å®Œæˆåå†™å…¥ç»“æœ
        if batch_records_to_save:
            append_to_parquet(batch_records_to_save, output_file)
            total_saved += len(batch_records_to_save)
            print(f"  ğŸ’¾ [Saved] {len(batch_records_to_save)} problems in this batch (Total: {total_saved} problems)")

        # ä¿å­˜checkpoint
        save_checkpoint(list(processed_problems), total_saved)
        print(f"  ğŸ’¾ [Checkpoint] Saved progress: {len(processed_problems)} problems processed")

        # æ£€æŸ¥å†…å­˜
        mem_mb = check_memory_usage()
        print(f"  ğŸ“Š Memory usage: {mem_mb:.1f} MB")

        del batch_df
        del batch_results
        del items_to_process
        gc.collect()
        print(f"\n  Batch {batch_num} completed. Total saved so far: {total_saved}")

    # æ¸…ç†DataFrame
    if lazy_df is not None:
        del lazy_df
    else:
        del df
    gc.collect()
    print(f"\nâœ“ Completed processing {dataset}\n")

# ä¿å­˜æœ€ç»ˆçŠ¶æ€
save_checkpoint(list(processed_problems), total_saved)

# æœ€ç»ˆæŠ¥å‘Š
final_mem_mb = get_memory_usage_mb()
print(f"\n{'='*80}")
print(f"âœ… Finished Multi-Version Test!")
print(f"{'='*80}")
print(f"Total saved: {total_saved} problems with passed incorrect submissions")
print(f"Total problems processed: {len(processed_problems)}")
print(f"Output directory: {output_dir}")

# åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶
output_files = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]
print(f"Generated {len(output_files)} output files:")
total_size = 0
for output_file in sorted(output_files):
    file_path = os.path.join(output_dir, output_file)
    if os.path.exists(file_path):
        file_size = os.path.getsize(file_path) / (1024 * 1024)
        total_size += file_size
        print(f"  - {output_file}: {file_size:.2f} MB")
print(f"Total output size: {total_size:.2f} MB")

print(f"Final memory usage: {final_mem_mb:.1f} MB")
print(f"{'='*80}")
